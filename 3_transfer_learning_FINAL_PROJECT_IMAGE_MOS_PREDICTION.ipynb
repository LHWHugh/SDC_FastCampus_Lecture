{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LHWHugh/SDC_FastCampus_Lecture/blob/main/3_transfer_learning_FINAL_PROJECT_IMAGE_MOS_PREDICTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYM61xrTsP5d"
      },
      "source": [
        "# 이미지 분류 - Transfer Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1otmJgmbahf"
      },
      "source": [
        "## 소개\n",
        "\n",
        "이미지 분류 모델에는 최소 수백만 개의 파라메터가 있습니다. 처음부터 train하려면 레이블이 맵핑된 많은 훈련 데이터와 컴퓨팅 리소스가 필요합니다. Transfer Learning(전이 학습)은 비슷한 문제에서 이미 훈련된 모델 (pretrained moodel)을 가져와 재사용함으로써 이러한 많은 자원 소모를 줄입니다.\n",
        "\n",
        "이미지 feature extraction을 위해 TensorFlow Hub에서 사전 훈련된 TF2 SavedModel(크고 일반적인 ImageNet 데이터에서 훈련완료)을 사용하여 5종의 꽃을 분류하기 위한 Keras 모델을 빌드하는 방법을 보여줍니다. (Optional) feature extractor를 새로 추가된 분류자와 함께 훈련(Fine tuning)할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL54LWCHt5q5"
      },
      "source": [
        "## 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrJHGnoyOhja"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow_hub==0.16.1\n",
        "#!pip install tf-keras==2.15.0\n",
        "#!pip install tensorflow==2.15.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlauq-4FWGZM",
        "outputId": "31a32c60-9519-4559-d7d0-56ba137cc0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.15.0\n",
            "Hub version: 0.16.1\n",
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import tf_keras as keras\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import urllib\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "0nO6xiOORGji"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract_data():\n",
        "    url = 'https://fastcampusjhpark.s3.ap-northeast-2.amazonaws.com/2023samsungAIchallenge_sampled.zip'\n",
        "    urllib.request.urlretrieve(url, '2023samsungAIchallenge.zip')\n",
        "    with zipfile.ZipFile('2023samsungAIchallenge.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "download_and_extract_data()"
      ],
      "metadata": {
        "id": "rzmT3_FzQ_W-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wd = \"./2023samsungAIchallenge_sampled/\"\n",
        "\n",
        "csv_file_path = wd + \"sampled_train.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "EW8H7RYIRB_p",
        "outputId": "c603e0fc-9c3c-495b-cb43-70befb3a338f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       img_name                img_path       mos  \\\n",
              "0    8wej0zjjcg  ./train/8wej0zjjcg.jpg  5.393064   \n",
              "1    7tp8y2wa0o  ./train/7tp8y2wa0o.jpg  5.668919   \n",
              "2    njj6uo8dl7  ./train/njj6uo8dl7.jpg  4.550725   \n",
              "3    gi7egofua1  ./train/gi7egofua1.jpg  5.661765   \n",
              "4    jfobhikir5  ./train/jfobhikir5.jpg  2.400901   \n",
              "..          ...                     ...       ...   \n",
              "995  q9l9o2ym98  ./train/q9l9o2ym98.jpg  5.679144   \n",
              "996  inuzo3y4sg  ./train/inuzo3y4sg.jpg  6.139535   \n",
              "997  d9chiie2q3  ./train/d9chiie2q3.jpg  5.955479   \n",
              "998  hr04byldlo  ./train/hr04byldlo.jpg  6.158537   \n",
              "999  oqe121r97w  ./train/oqe121r97w.jpg  6.387435   \n",
              "\n",
              "                                              comments  \n",
              "0    lovely photo, maybe a bit too dark because of ...  \n",
              "1    i really like the way you have done these laye...  \n",
              "2    cute and classy! i like this a lot especially ...  \n",
              "3    nice and spooky, and you fit the bill for one ...  \n",
              "4    im really trying to get this, but i dont. blue...  \n",
              "..                                                 ...  \n",
              "995  one of my s for the week. the colors are almos...  \n",
              "996  lovely effect on the flowers with the backligh...  \n",
              "997  very good! i love the grainyedgy look to it, b...  \n",
              "998  very cute, and the colours are so warm, i can ...  \n",
              "999  truly eerie. the use of light and shadows is d...  \n",
              "\n",
              "[1000 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-de08e7bf-ebf7-497a-a6a1-99279c6b9806\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_name</th>\n",
              "      <th>img_path</th>\n",
              "      <th>mos</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8wej0zjjcg</td>\n",
              "      <td>./train/8wej0zjjcg.jpg</td>\n",
              "      <td>5.393064</td>\n",
              "      <td>lovely photo, maybe a bit too dark because of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7tp8y2wa0o</td>\n",
              "      <td>./train/7tp8y2wa0o.jpg</td>\n",
              "      <td>5.668919</td>\n",
              "      <td>i really like the way you have done these laye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>njj6uo8dl7</td>\n",
              "      <td>./train/njj6uo8dl7.jpg</td>\n",
              "      <td>4.550725</td>\n",
              "      <td>cute and classy! i like this a lot especially ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gi7egofua1</td>\n",
              "      <td>./train/gi7egofua1.jpg</td>\n",
              "      <td>5.661765</td>\n",
              "      <td>nice and spooky, and you fit the bill for one ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jfobhikir5</td>\n",
              "      <td>./train/jfobhikir5.jpg</td>\n",
              "      <td>2.400901</td>\n",
              "      <td>im really trying to get this, but i dont. blue...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>q9l9o2ym98</td>\n",
              "      <td>./train/q9l9o2ym98.jpg</td>\n",
              "      <td>5.679144</td>\n",
              "      <td>one of my s for the week. the colors are almos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>inuzo3y4sg</td>\n",
              "      <td>./train/inuzo3y4sg.jpg</td>\n",
              "      <td>6.139535</td>\n",
              "      <td>lovely effect on the flowers with the backligh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>d9chiie2q3</td>\n",
              "      <td>./train/d9chiie2q3.jpg</td>\n",
              "      <td>5.955479</td>\n",
              "      <td>very good! i love the grainyedgy look to it, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>hr04byldlo</td>\n",
              "      <td>./train/hr04byldlo.jpg</td>\n",
              "      <td>6.158537</td>\n",
              "      <td>very cute, and the colours are so warm, i can ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>oqe121r97w</td>\n",
              "      <td>./train/oqe121r97w.jpg</td>\n",
              "      <td>6.387435</td>\n",
              "      <td>truly eerie. the use of light and shadows is d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de08e7bf-ebf7-497a-a6a1-99279c6b9806')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-de08e7bf-ebf7-497a-a6a1-99279c6b9806 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-de08e7bf-ebf7-497a-a6a1-99279c6b9806');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fd618ae1-3171-431e-9814-516544c6f619\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fd618ae1-3171-431e-9814-516544c6f619')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fd618ae1-3171-431e-9814-516544c6f619 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8952f0e7-c8cb-4ed7-9e7a-131a58415176\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8952f0e7-c8cb-4ed7-9e7a-131a58415176 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"img_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"9pkevimfuy\",\n          \"q1ys00qrvg\",\n          \"kr4b088224\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"img_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"./train/9pkevimfuy.jpg\",\n          \"./train/q1ys00qrvg.jpg\",\n          \"./train/kr4b088224.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mos\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8608248326859594,\n        \"min\": 2.02413793103,\n        \"max\": 8.23270440252,\n        \"num_unique_values\": 984,\n        \"samples\": [\n          5.74789915966,\n          6.68609865471,\n          6.29249011858\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 998,\n        \"samples\": [\n          \"what a stunning capture. i love the sepia treatment as well. excellent!\",\n          \"love the streams of light. great exposure.\",\n          \"a bit gross, but wonderfully caught! excellent clarity and detail.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmaHHH7Pvmth"
      },
      "source": [
        "## 사용할 TF2 SavedModel 모듈 선택하기\n",
        "\n",
        "(참고) [https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4)를 사용하세요.  \n",
        "동일한 URL을 코드에 사용하여 SavedModel을 식별하고 브라우저에서 설명서를 표시할 수 있습니다. 참고로 TF1 Hub 형식의 모델은 여기서 작동하지 않습니다.\n",
        "\n",
        "[여기](https://tfhub.dev/s?module-type=image-feature-vector&tf-version=tf2)에서 이미지 특성 벡터 (feature vector)를 생성하는 더 많은 TF2 모델을 찾을 수 있습니다.\n",
        "\n",
        "시도할 수 있는 여러 모델이 있습니다. 아래 셀에서 다른 모델을 선택하고 노트북을 사용하기만 하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlsEcKVeuCnf",
        "outputId": "51d0cd4a-4ecf-411a-b11a-04c0ef1845ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected model: resnet_v2_50 : https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\n",
            "Input size (224, 224)\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "\n",
        "model_name = \"resnet_v2_50\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
        "\n",
        "model_handle_map = {\n",
        "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
        "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
        "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
        "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
        "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
        "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
        "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
        "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
        "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
        "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
        "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
        "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
        "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
        "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
        "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
        "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
        "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
        "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
        "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
        "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
        "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
        "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
        "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
        "}\n",
        "\n",
        "model_image_size_map = {\n",
        "  \"efficientnetv2-s\": 384,\n",
        "  \"efficientnetv2-m\": 480,\n",
        "  \"efficientnetv2-l\": 480,\n",
        "  \"efficientnetv2-b0\": 224,\n",
        "  \"efficientnetv2-b1\": 240,\n",
        "  \"efficientnetv2-b2\": 260,\n",
        "  \"efficientnetv2-b3\": 300,\n",
        "  \"efficientnetv2-s-21k\": 384,\n",
        "  \"efficientnetv2-m-21k\": 480,\n",
        "  \"efficientnetv2-l-21k\": 480,\n",
        "  \"efficientnetv2-xl-21k\": 512,\n",
        "  \"efficientnetv2-b0-21k\": 224,\n",
        "  \"efficientnetv2-b1-21k\": 240,\n",
        "  \"efficientnetv2-b2-21k\": 260,\n",
        "  \"efficientnetv2-b3-21k\": 300,\n",
        "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
        "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
        "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
        "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
        "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
        "  \"efficientnetv2-b3-21k-ft1k\": 300,\n",
        "  \"efficientnet_b0\": 224,\n",
        "  \"efficientnet_b1\": 240,\n",
        "  \"efficientnet_b2\": 260,\n",
        "  \"efficientnet_b3\": 300,\n",
        "  \"efficientnet_b4\": 380,\n",
        "  \"efficientnet_b5\": 456,\n",
        "  \"efficientnet_b6\": 528,\n",
        "  \"efficientnet_b7\": 600,\n",
        "  \"inception_v3\": 299,\n",
        "  \"inception_resnet_v2\": 299,\n",
        "  \"nasnet_large\": 331,\n",
        "  \"pnasnet_large\": 331,\n",
        "}\n",
        "\n",
        "model_handle = model_handle_map.get(model_name)\n",
        "pixels = model_image_size_map.get(model_name, 224)\n",
        "\n",
        "print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(f\"Input size {IMAGE_SIZE}\")\n",
        "\n",
        "BATCH_SIZE = 16#@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTY8qzyYv3vl"
      },
      "source": [
        "## Flowers 데이터세트 설정하기\n",
        "\n",
        "입력은 선택한 모듈에 맞게 적절한 크기로 조정됩니다. 데이터세트 확대(즉, 읽을 때마다 이미지의 임의 왜곡)로 특히 미세 조정할 때 훈련이 개선됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBtFK1hO8KsO"
      },
      "outputs": [],
      "source": [
        "data_dir = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTeOnApqOhjg"
      },
      "outputs": [],
      "source": [
        "data_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGERmMHeOhjg"
      },
      "outputs": [],
      "source": [
        "do_data_augmentation = False #@param {type:\"boolean\"}\n",
        "\n",
        "def build_dataset(subset, do_data_augmentation):\n",
        "    # Initialize ImageDataGenerator with the appropriate subset split and preprocessing\n",
        "    if do_data_augmentation:\n",
        "        datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=40,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            horizontal_flip=True,\n",
        "            validation_split=0.20\n",
        "        )\n",
        "    else:\n",
        "        datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            validation_split=0.20\n",
        "        )\n",
        "\n",
        "    # Generate batches of image data and labels\n",
        "    return datagen.flow_from_directory(\n",
        "        directory=data_dir,\n",
        "        target_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        subset=subset,  # Specify 'training' or 'validation' subset\n",
        "        seed=123  # Seed for reproducibility\n",
        "    )\n",
        "\n",
        "# Build training and validation datasets\n",
        "train_ds = build_dataset(\"training\", do_data_augmentation)\n",
        "val_ds = build_dataset(\"validation\", False)\n",
        "\n",
        "# Capture class names from the directory structure\n",
        "class_names = train_ds.class_indices\n",
        "\n",
        "# Retrieve dataset sizes\n",
        "train_size = train_ds.samples\n",
        "valid_size = val_ds.samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umB5tswsfTEQ"
      },
      "outputs": [],
      "source": [
        "def build_dataset(subset):\n",
        "  return tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      data_dir,\n",
        "      validation_split=.20,\n",
        "      subset=subset,\n",
        "      label_mode=\"categorical\",\n",
        "      # Seed needs to provided when using validation_split and shuffle = True.\n",
        "      # A fixed seed is used so that the validation set is stable across runs.\n",
        "      seed=123,\n",
        "      image_size=IMAGE_SIZE,\n",
        "      batch_size=1)\n",
        "\n",
        "train_ds = build_dataset(\"training\")\n",
        "class_names = tuple(train_ds.class_names)\n",
        "train_size = train_ds.cardinality().numpy()\n",
        "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
        "train_ds = train_ds.repeat()\n",
        "\n",
        "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
        "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
        "do_data_augmentation = False #@param {type:\"boolean\"}\n",
        "if do_data_augmentation:\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomRotation(40))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomTranslation(0, 0.2))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomTranslation(0.2, 0))\n",
        "  # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n",
        "  # image sizes are fixed when reading, and then a random zoom is applied.\n",
        "  # If all training inputs are larger than image_size, one could also use\n",
        "  # RandomCrop with a batch size of 1 and rebatch later.\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomZoom(0.2, 0.2))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
        "train_ds = train_ds.map(lambda images, labels:\n",
        "                        (preprocessing_model(images), labels))\n",
        "\n",
        "val_ds = build_dataset(\"validation\")\n",
        "valid_size = val_ds.cardinality().numpy()\n",
        "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
        "val_ds = val_ds.map(lambda images, labels:\n",
        "                    (normalization_layer(images), labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS_gVStowW3G"
      },
      "source": [
        "## 모델 정의하기\n",
        "\n",
        "Hub 모듈을 사용하여 `feature_extractor_layer` 위에 classifier를 배치하기만 하면 됩니다.\n",
        "\n",
        "속도를 높이기 위해 훈련할 수 없는 `feature_extractor_layer`로 시작하지만 정확성을 높이기 위해 미세 조정 (fine-tuning)을 사용할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaJW3XrPyFiF"
      },
      "outputs": [],
      "source": [
        "do_fine_tuning = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50FYNIb1dmJH"
      },
      "outputs": [],
      "source": [
        "print(\"Building model with\", model_handle)\n",
        "model = keras.Sequential([\n",
        "    # Explicitly define the input shape so the model can be properly\n",
        "    # loaded by the TFLiteConverter\n",
        "    keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(len(class_names), activation='softmax'\n",
        "                      #    ,kernel_regularizer=tf.keras.regularizers.l2(0.0001)\n",
        "                      )\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-D3ewEvOhjj"
      },
      "source": [
        "# 학습 전 예상 확인하기\n",
        "\n",
        "  - 아직 transfer learning 을 하기 전입니다. 예상을 잘 할 수 있을까요?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7c7FvVdOhjj"
      },
      "outputs": [],
      "source": [
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj-b5cycOhjk"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(val_ds))\n",
        "image = x[0, :, :, :]\n",
        "true_index = np.argmax(y[0])\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
        "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
        "predicted_index = np.argmax(prediction_scores)\n",
        "print(\"True label: \" + str(true_index))\n",
        "print(\"Predicted label: \" + str(predicted_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2e5WupIw2N2"
      },
      "source": [
        "## 모델 훈련하기\n",
        "\n",
        "  - 시간 계산 필요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f3yBUvkd_VJ"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_YKX2Qnfg6x"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = train_size // BATCH_SIZE\n",
        "validation_steps = valid_size // BATCH_SIZE\n",
        "hist = model.fit(\n",
        "    train_ds,\n",
        "    epochs=5, steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=validation_steps).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYOw0fTO1W4x"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ScitaPqhKtuW"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}